{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw1 import ValueNetwork, Utility_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.014s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestValueNetwork(unittest.TestCase):\n",
    "    \"\"\" Test the ValueNetwork class \"\"\"\n",
    "    def setUp(self):\n",
    "        \"\"\"Initialize ValueNetwork\"\"\"\n",
    "        self.input_dim = 2\n",
    "        self.hidden_dim = 32\n",
    "        self.model = ValueNetwork(self.input_dim, self.hidden_dim)\n",
    "\n",
    "    def test_forward_pass(self):\n",
    "        \"\"\" Test the forward pass of the ValueNetwork \"\"\"\n",
    "        x = torch.randn(5, self.input_dim)  # batch_size=5\n",
    "        output = self.model(x)\n",
    "\n",
    "        # Make sure the output has the correct shape\n",
    "        self.assertEqual(output.shape, (5, 1))\n",
    "\n",
    "    def test_parameter_update(self):\n",
    "        \"\"\" Make sure the parameters of the ValueNetwork are updated \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01) # optimizer with learning rate 0.01\n",
    "        x = torch.randn(5, self.input_dim) \n",
    "        target = torch.randn(5, 1) # target values\n",
    "        criterion = torch.nn.MSELoss() # mean squared error loss\n",
    "\n",
    "        output = self.model(x)\n",
    "        loss = criterion(output, target) # compute the loss\n",
    "        loss.backward() # compute the gradients\n",
    "\n",
    "        # record the parameters before the update\n",
    "        params_before = [p.clone().detach() for p in self.model.parameters()]\n",
    "        optimizer.step() # update the parameters\n",
    "\n",
    "        # record the parameters after the update\n",
    "        params_after = [p.clone().detach() for p in self.model.parameters()]\n",
    "        for p_before, p_after in zip(params_before, params_after):\n",
    "            self.assertFalse(torch.equal(p_before, p_after))  # make sure the parameters have changed\n",
    "\n",
    "    def test_Utility_function(self):\n",
    "        \"\"\" test the Utility_function function \"\"\"\n",
    "        W_T = np.array([10, 20, 30]) \n",
    "        a = 1 # a is a scalar\n",
    "        expected_values = -np.exp(-a * W_T) / a \n",
    "        computed_values = Utility_function(W_T, a) \n",
    "        # make sure the computed values are close to the expected values\n",
    "        np.testing.assert_almost_equal(computed_values, expected_values)\n",
    "\n",
    "# run the tests\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
